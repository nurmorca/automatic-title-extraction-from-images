{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":703974,"sourceType":"datasetVersion","datasetId":359207},{"sourceId":11824946,"sourceType":"datasetVersion","datasetId":7428193},{"sourceId":428051,"sourceType":"modelInstanceVersion","modelInstanceId":348942,"modelId":370204},{"sourceId":428065,"sourceType":"modelInstanceVersion","modelInstanceId":348952,"modelId":370214},{"sourceId":428075,"sourceType":"modelInstanceVersion","modelInstanceId":348961,"modelId":370225},{"sourceId":428080,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":348965,"modelId":370229},{"sourceId":430171,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350658,"modelId":371909},{"sourceId":430183,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350669,"modelId":371919},{"sourceId":430190,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":350675,"modelId":371926},{"sourceId":430697,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":351062,"modelId":372314}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## install & imports","metadata":{}},{"cell_type":"code","source":"!apt-get update -qq\n!apt-get install -qq -y wget unzip\n\n!pip install --quiet gradio transformers torch pillow pyngrok\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:57:00.039716Z","iopub.execute_input":"2025-08-25T17:57:00.039983Z","iopub.status.idle":"2025-08-25T17:58:35.502600Z","shell.execute_reply.started":"2025-08-25T17:57:00.039960Z","shell.execute_reply":"2025-08-25T17:58:35.501816Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.6/324.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.0/73.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader\nfrom torchvision.models import ResNet50_Weights, VGG19_Weights, VGG16_Weights, Inception_V3_Weights, inception_v3\nfrom torch.cuda.amp import GradScaler, autocast\nfrom datetime import datetime\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pyngrok import ngrok\nimport gradio as gr\nimport re\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport threading\nimport torch.nn.functional as F\nimport numpy as np\nfrom transformers import BertTokenizerFast # BERT tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:58:35.504272Z","iopub.execute_input":"2025-08-25T17:58:35.504536Z","iopub.status.idle":"2025-08-25T17:59:02.550645Z","shell.execute_reply.started":"2025-08-25T17:58:35.504512Z","shell.execute_reply":"2025-08-25T17:59:02.550038Z"}},"outputs":[{"name":"stderr","text":"2025-08-25 17:58:44.975521: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756144725.174311      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756144725.226624      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"NGROK_AUTH_TOKEN = #PASTE YOUR OWN TOKEN\n\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\nprint(\"ngrok authtoken configured.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.551355Z","iopub.execute_input":"2025-08-25T17:59:02.551888Z","iopub.status.idle":"2025-08-25T17:59:02.624106Z","shell.execute_reply.started":"2025-08-25T17:59:02.551869Z","shell.execute_reply":"2025-08-25T17:59:02.623191Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## vocab","metadata":{}},{"cell_type":"code","source":"TRAIN_ANNOTATIONS_FOR_VOCAB = \"/kaggle/input/all-data-coco/turkish_captions_train (1).csv\"\nFREQ_THRESHOLD_FOR_VOCAB = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.626615Z","iopub.execute_input":"2025-08-25T17:59:02.626907Z","iopub.status.idle":"2025-08-25T17:59:02.656860Z","shell.execute_reply.started":"2025-08-25T17:59:02.626879Z","shell.execute_reply":"2025-08-25T17:59:02.655982Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def preprocess_caption_turkish(caption):\n    caption = caption.strip()\n    caption = caption.replace(\"I\", \"ı\").replace(\"İ\", \"i\").lower()\n    caption = re.sub(r\"[^a-z0-9ğüşıöç\\s]\", \"\", caption, flags=re.UNICODE)\n    caption = re.sub(r'\\s+', ' ', caption)\n    return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.657741Z","iopub.execute_input":"2025-08-25T17:59:02.658234Z","iopub.status.idle":"2025-08-25T17:59:02.675585Z","shell.execute_reply.started":"2025-08-25T17:59:02.658206Z","shell.execute_reply":"2025-08-25T17:59:02.674751Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self, freq_threshold=1):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n\n    def __len__(self):\n        return len(self.itos)\n\n    @staticmethod\n    def tokenizer(text):\n        if not isinstance(text, str):\n            return []\n        return text.lower().split()\n\n    def build_vocabulary(self, sentence_list_raw):\n        frequencies = {}\n        idx = len(self.itos)\n\n        print(f\"Kelime dağarcığı için {len(sentence_list_raw)} ham cümle işleniyor...\")\n        for sentence in tqdm(sentence_list_raw, desc=\"Kelime dağarcığı oluşturuluyor\"):\n            cleaned_sentence = preprocess_caption_turkish(sentence)\n            for token in cleaned_sentence.split(): \n                if token: # Boş string tokenları (eğer oluşursa) atla\n                    frequencies[token] = frequencies.get(token, 0) + 1\n        \n        print(\"Frekanslar hesaplandı. Kelimeler eşiğe göre kelime dağarcığına ekleniyor...\")\n        for token, count in frequencies.items():\n            if count >= self.freq_threshold: \n                if token not in self.stoi:\n                    self.stoi[token] = idx\n                    self.itos[idx] = token\n                    idx += 1\n        print(f\"Kelime dağarcığı oluşturma tamamlandı. Toplam boyut (özel tokenlar dahil): {len(self.itos)}\")\n        print(f\"Eklenen kelime sayısı (özel tokenlar hariç): {idx - 4}\")\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenizer(text)\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n            for token in tokenized_text\n        ]\n\n\n\nclass CocoDataset(Dataset):\n    def __init__(self, root_dir, captions_file, transform=None, vocab=None, \n                 freq_threshold_for_build=2, secondary_dir=None, max_samples=None): # vocab eklendi\n        self.root_dir = root_dir\n        self.secondary_dir = secondary_dir\n        self.transform = transform\n        self.captions_file_path = captions_file\n        \n        self.df = pd.read_csv(\n            captions_file, on_bad_lines='skip', engine='python', encoding='utf-8'\n        )\n        self.df.dropna(subset=[\"Türkçe_açıklama\"], inplace=True)\n        self.df[\"filename\"] = self.df[\"Coco_url\"].apply(\n            lambda x: x.split(\"/\")[-1] if isinstance(x, str) else \"\"\n        )\n        \n        self.df = self.df[self.df[\"filename\"] != \"\"].copy()\n\n        if max_samples is not None and max_samples > 0:\n            if max_samples < len(self.df):\n                self.df = self.df.head(max_samples).reset_index(drop=True)\n\n        self.imgs = self.df[\"filename\"]\n        self.captions = self.df[\"Türkçe_açıklama\"]\n\n        if vocab is not None:\n            self.vocab = vocab\n        else:\n            print(f\"'{self.captions_file_path}' üzerinden YENİ kelime dağarcığı oluşturuluyor (freq_threshold={freq_threshold_for_build})...\")\n            self.vocab = Vocabulary(freq_threshold_for_build)\n            self.vocab.build_vocabulary(self.captions.tolist())\n            print(f\"Yeni kelime dağarcığı oluşturuldu. Boyut: {len(self.vocab)}\")\n            \n    # __len__ ve __getitem__ metodları aynı kalabilir\n    # ... (geri kalan CocoDataset metodları)\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        caption = self.captions.iloc[index]\n        img_id  = self.imgs.iloc[index]\n\n        img_path = os.path.join(self.root_dir, img_id)\n        if not os.path.exists(img_path) and self.secondary_dir is not None:\n            img_path = os.path.join(self.secondary_dir, img_id)\n        \n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"UYARI: Görüntü bulunamadı {img_path}\")\n            if index + 1 < len(self.df):\n                print(f\"{img_path} bulunamadı, bir sonraki örneğe geçiliyor.\")\n                return self.__getitem__(index + 1)\n            else:\n                raise FileNotFoundError(f\"Kritik: Görüntü {img_path} bulunamadı ve başka örnek yok.\")\n\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        new_cap = preprocess_caption_turkish(caption)\n        numericalized_caption = (\n            [self.vocab.stoi[\"<SOS>\"]]\n            + self.vocab.numericalize(new_cap) # self.vocab artık paylaşılan vocab olacak\n            + [self.vocab.stoi[\"<EOS>\"]]\n        )\n        return img, torch.tensor(numericalized_caption, dtype=torch.long)\n\nclass MyCollate:\n    def __init__(self, pad_token_id):\n        self.pad_token_id = pad_token_id\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_token_id)\n        return imgs, targets\n\n\ndef get_loader(\n    root_folder,\n    secondary_folder,\n    annotation_file,\n    transform,\n    batch_size=64,\n    num_workers=8,\n    shuffle=True,\n    pin_memory=True,\n    max_samples=None,\n    vocab=None,\n    freq_threshold_for_build=5 \n):\n    dataset = CocoDataset(\n        root_dir=root_folder,\n        captions_file=annotation_file,\n        transform=transform,\n        secondary_dir=secondary_folder,\n        max_samples=max_samples,\n        vocab=vocab,\n        freq_threshold_for_build=freq_threshold_for_build\n    )\n    \n    if len(dataset) == 0:\n        print(f\"UYARI: {annotation_file} için oluşturulan veri seti BOŞ! max_samples={max_samples}\")\n    \n    pad_token_id = dataset.vocab.stoi[\"<PAD>\"]\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollate(pad_token_id=pad_token_id),\n    )\n    return loader, dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.676569Z","iopub.execute_input":"2025-08-25T17:59:02.676785Z","iopub.status.idle":"2025-08-25T17:59:02.696588Z","shell.execute_reply.started":"2025-08-25T17:59:02.676769Z","shell.execute_reply":"2025-08-25T17:59:02.695915Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def load_vocab_from_train_file(annotations_file, freq_threshold, isRep = None):\n    print(f\"'{annotations_file}' üzerinden kelime dağarcığı yeniden oluşturuluyor...\")\n    df = pd.read_csv(annotations_file, on_bad_lines='skip', engine='python', encoding='utf-8')\n    df.dropna(subset=[\"Türkçe_açıklama\"], inplace=True)\n    \n    df[\"filename\"] = df[\"Coco_url\"].apply(\n        lambda x: x.split(\"/\")[-1] if isinstance(x, str) else \"\"\n    )\n    df = df[df[\"filename\"] != \"\"].copy()\n    if isRep == \"reppen\":\n        df = df.drop_duplicates(subset=[\"filename\"], keep=\"first\").reset_index(drop=True)\n        \n    captions_list = df[\"Türkçe_açıklama\"].tolist()\n    \n    vocab = Vocabulary(freq_threshold)\n    vocab.build_vocabulary(captions_list)\n    print(f\"Kelime dağarcığı oluşturuldu. Boyut: {len(vocab)}\")\n    return vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.697286Z","iopub.execute_input":"2025-08-25T17:59:02.697509Z","iopub.status.idle":"2025-08-25T17:59:02.719630Z","shell.execute_reply.started":"2025-08-25T17:59:02.697493Z","shell.execute_reply":"2025-08-25T17:59:02.719008Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\nvocabulary = load_vocab_from_train_file(TRAIN_ANNOTATIONS_FOR_VOCAB, FREQ_THRESHOLD_FOR_VOCAB)\nrep_vocabulary = load_vocab_from_train_file(TRAIN_ANNOTATIONS_FOR_VOCAB, FREQ_THRESHOLD_FOR_VOCAB, \"reppen\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:02.720488Z","iopub.execute_input":"2025-08-25T17:59:02.721229Z","iopub.status.idle":"2025-08-25T17:59:13.908898Z","shell.execute_reply.started":"2025-08-25T17:59:02.721200Z","shell.execute_reply":"2025-08-25T17:59:13.908191Z"}},"outputs":[{"name":"stdout","text":"'/kaggle/input/all-data-coco/turkish_captions_train (1).csv' üzerinden kelime dağarcığı yeniden oluşturuluyor...\nKelime dağarcığı için 336496 ham cümle işleniyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Kelime dağarcığı oluşturuluyor:   0%|          | 0/336496 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad1986f75134c43af3adafced5b90d1"}},"metadata":{}},{"name":"stdout","text":"Frekanslar hesaplandı. Kelimeler eşiğe göre kelime dağarcığına ekleniyor...\nKelime dağarcığı oluşturma tamamlandı. Toplam boyut (özel tokenlar dahil): 23233\nEklenen kelime sayısı (özel tokenlar hariç): 23229\nKelime dağarcığı oluşturuldu. Boyut: 23233\n'/kaggle/input/all-data-coco/turkish_captions_train (1).csv' üzerinden kelime dağarcığı yeniden oluşturuluyor...\nKelime dağarcığı için 75214 ham cümle işleniyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Kelime dağarcığı oluşturuluyor:   0%|          | 0/75214 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7f8e99daf249c7b9a818a774007f0d"}},"metadata":{}},{"name":"stdout","text":"Frekanslar hesaplandı. Kelimeler eşiğe göre kelime dağarcığına ekleniyor...\nKelime dağarcığı oluşturma tamamlandı. Toplam boyut (özel tokenlar dahil): 11528\nEklenen kelime sayısı (özel tokenlar hariç): 11524\nKelime dağarcığı oluşturuldu. Boyut: 11528\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## tokenizer","metadata":{}},{"cell_type":"code","source":"BERT_MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n\ndef preprocess_caption_for_bert_turkish(caption: str) -> str:\n    caption = caption.strip()\n    caption = caption.replace(\"I\", \"ı\").replace(\"İ\", \"i\")\n    return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:13.909817Z","iopub.execute_input":"2025-08-25T17:59:13.910088Z","iopub.status.idle":"2025-08-25T17:59:13.914313Z","shell.execute_reply.started":"2025-08-25T17:59:13.910063Z","shell.execute_reply":"2025-08-25T17:59:13.913497Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# tokenizer'li her seyin sonuna T ekledik. \nclass VocabularyT:\n    def __init__(self, tokenizer_name: str = BERT_MODEL_NAME):\n        print(f\"'{tokenizer_name}' BERT tokenizer yükleniyor...\")\n        try:\n            self.tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n        except Exception as e:\n            print(f\"HATA: BERT tokenizer '{tokenizer_name}' yüklenemedi. Hata: {e}\")\n            print(\"Lütfen 'transformers' kütüphanesinin kurulu ve model adının doğru olduğundan emin olun.\")\n            raise\n\n        self.pad_token = self.tokenizer.pad_token\n        self.sos_token = self.tokenizer.cls_token\n        self.eos_token = self.tokenizer.sep_token \n        self.unk_token = self.tokenizer.unk_token\n\n        self.pad_token_id = self.tokenizer.pad_token_id\n        self.sos_token_id = self.tokenizer.cls_token_id\n        self.eos_token_id = self.tokenizer.sep_token_id\n        self.unk_token_id = self.tokenizer.unk_token_id\n\n        print(f\"BERT Tokenizer '{tokenizer_name}' başarıyla yüklendi.\")\n        print(f\"  PAD token: '{self.pad_token}' (ID: {self.pad_token_id})\")\n        print(f\"  SOS/CLS token: '{self.sos_token}' (ID: {self.sos_token_id})\")\n        print(f\"  EOS/SEP token: '{self.eos_token}' (ID: {self.eos_token_id})\")\n        print(f\"  UNK token: '{self.unk_token}' (ID: {self.unk_token_id})\")\n        print(f\"  Vocabulary size: {self.tokenizer.vocab_size}\")\n\n        self._itos_list = None \n\n    def __len__(self):\n        return self.tokenizer.vocab_size\n\n    def numericalize(self, text_processed_for_bert: str, add_special_tokens: bool = False) -> list[int]:\n        token_ids = self.tokenizer.encode(\n            text_processed_for_bert,\n            add_special_tokens=add_special_tokens,\n            truncation=False, # Truncation ve max_length genellikle encode_plus'ta yönetilir\n        )\n        return token_ids\n\n    @property\n    def itos(self):\n        if self._itos_list is None:\n            self._itos_list = self.tokenizer.convert_ids_to_tokens(list(range(self.tokenizer.vocab_size)))\n        return self._itos_list\n\n    @property\n    def stoi(self):\n        return self.tokenizer.get_vocab()\n    def decode_ids_to_string(self, token_ids: list[int]) -> str:\n        return self.tokenizer.decode(token_ids, skip_special_tokens=True)\n\nclass CocoDatasetT(Dataset):\n    def __init__(self, root_dir: str, captions_file: str, transform=None, vocab_tokenizer: VocabularyT = None,\n                 secondary_dir: str = None, max_samples: int = None, max_caption_len: int = 100):\n        self.root_dir = root_dir\n        self.secondary_dir = secondary_dir\n        self.transform = transform\n        self.captions_file_path = captions_file\n        self.max_caption_len = max_caption_len\n\n        df_initial = pd.read_csv(\n            captions_file, on_bad_lines='skip', engine='python', encoding='utf-8'\n        )\n        df_initial.dropna(subset=[\"Türkçe_açıklama\"], inplace=True)\n\n        if \"filename\" not in df_initial.columns and \"Coco_url\" in df_initial.columns:\n            df_initial[\"filename\"] = df_initial[\"Coco_url\"].apply(\n                lambda x: x.split(\"/\")[-1] if isinstance(x, str) else \"\"\n            )\n        elif \"filename\" not in df_initial.columns and \"image\" in df_initial.columns: # Flickr8k benzeri format için\n            df_initial = df_initial.rename(columns={\"image\": \"filename\"})\n\n        df_initial = df_initial[df_initial[\"filename\"] != \"\"].copy()\n\n        self.df = df_initial.reset_index(drop=True)\n\n        if max_samples is not None and max_samples > 0:\n            # max_samples artık toplam başlık sayısı üzerinden uygulanacak.\n            if max_samples < len(self.df):\n                self.df = self.df.head(max_samples).reset_index(drop=True)\n        \n        print(f\"Veri kümesi hazırlandı. Toplam görsel-başlık çifti sayısı: {len(self.df)}\")\n\n        self.imgs_filenames = self.df[\"filename\"].tolist()\n        self.raw_captions_list = self.df[\"Türkçe_açıklama\"].tolist()\n\n        if vocab_tokenizer is not None:\n            self.vocab_tokenizer = vocab_tokenizer\n        else:\n            print(f\"'{self.captions_file_path}' için YENİ BERT tokenizer ve vocab oluşturuluyor (CocoDataset içinde)...\")\n            # BERT_MODEL_NAME değişkeninin tanımlı olması gerekir\n            self.vocab_tokenizer = VocabularyT(tokenizer_name=BERT_MODEL_NAME)\n            \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index: int):\n        caption_text_raw = self.raw_captions_list[index]\n        img_filename = self.imgs_filenames[index]\n\n        img_path = os.path.join(self.root_dir, img_filename)\n        if self.secondary_dir and not os.path.exists(img_path):\n            img_path = os.path.join(self.secondary_dir, img_filename)\n\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except FileNotFoundError:\n            print(f\"UYARI: __getitem__ içinde görüntü bulunamadı: {img_path}.\")\n            raise FileNotFoundError(f\"Görüntü yüklenemedi: {img_path}\")\n\n        if self.transform is not None:\n            img = self.transform(img)\n            \n        processed_caption_for_bert = preprocess_caption_for_bert_turkish(caption_text_raw)\n        \n        encoding = self.vocab_tokenizer.tokenizer.encode_plus(\n            processed_caption_for_bert,\n            add_special_tokens=True,\n            max_length=self.max_caption_len,\n            padding='do_not_pad', # Padding collate_fn'de yapılacak\n            truncation=True,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            return_tensors=None \n        )\n        numericalized_caption_ids = encoding['input_ids']\n\n        return img, torch.tensor(numericalized_caption_ids, dtype=torch.long)\n\nclass MyCollateT:\n    def __init__(self, pad_token_id: int):\n        self.pad_token_id = pad_token_id\n\n    def __call__(self, batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs, dim=0)\n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_token_id)\n        return imgs, targets\n\ndef get_loader(\n    root_folder: str,\n    secondary_folder: str,\n    annotation_file: str,\n    transform,\n    batch_size: int = 64,\n    num_workers: int = 8,\n    shuffle: bool = True,\n    pin_memory: bool = True,\n    max_samples: int = None,\n    vocab_tokenizer: VocabularyT = None,\n    max_caption_len: int = 100      \n):\n    dataset = CocoDatasetT(\n        root_dir=root_folder,\n        captions_file=annotation_file,\n        transform=transform,\n        secondary_dir=secondary_folder,\n        max_samples=max_samples,\n        vocab_tokenizer=vocab_tokenizer,\n        max_caption_len=max_caption_len\n    )\n\n    if len(dataset) == 0:\n        print(f\"UYARI: {annotation_file} için oluşturulan veri seti BOŞ! max_samples={max_samples}\")\n        return None, None\n\n    pad_token_id = dataset.vocab_tokenizer.pad_token_id\n\n    loader = DataLoader(\n        dataset=dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=pin_memory,\n        collate_fn=MyCollateT(pad_token_id=pad_token_id),\n    )\n    return loader, dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:13.916691Z","iopub.execute_input":"2025-08-25T17:59:13.916901Z","iopub.status.idle":"2025-08-25T17:59:13.950645Z","shell.execute_reply.started":"2025-08-25T17:59:13.916884Z","shell.execute_reply":"2025-08-25T17:59:13.950043Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"vocabT = VocabularyT(BERT_MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:13.951358Z","iopub.execute_input":"2025-08-25T17:59:13.951545Z","iopub.status.idle":"2025-08-25T17:59:16.084748Z","shell.execute_reply.started":"2025-08-25T17:59:13.951531Z","shell.execute_reply":"2025-08-25T17:59:16.083972Z"}},"outputs":[{"name":"stdout","text":"'dbmdz/bert-base-turkish-cased' BERT tokenizer yükleniyor...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b9e4cd75e34e7c9a55fffbd56b9987"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0921b8d21a7042529631075322ea1571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"783ea5c34e3843b1ab64e271aef3157e"}},"metadata":{}},{"name":"stdout","text":"BERT Tokenizer 'dbmdz/bert-base-turkish-cased' başarıyla yüklendi.\n  PAD token: '[PAD]' (ID: 0)\n  SOS/CLS token: '[CLS]' (ID: 2)\n  EOS/SEP token: '[SEP]' (ID: 3)\n  UNK token: '[UNK]' (ID: 1)\n  Vocabulary size: 32000\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## models\n","metadata":{}},{"cell_type":"code","source":"# CHECKPOINTS FOR MODELS\nBNL_MINI_CHECKPOINT = \"/kaggle/input/inception_lstm_vocab/pytorch/default/1/inception_lstm_vocab.pth\"\nBNL_LARGE_CHECKPOINT = \"/kaggle/input/resnet50_lstm_combined/pytorch/default/1/resnet50_combined.pth\" \nBNL_MID_CHECKPOINT = \"/kaggle/input/vgg16_lstm_combined/pytorch/default/1/vgg16_combined_model.pth\"\nBNL_LARGE_REP_CHECKPOINT = \"/kaggle/input/rep_penalty/pytorch/default/1/mscoco_checkpoint_best(7).pth\"\nBNL_LARGE_TURBO_CHECKPOINT = \"/kaggle/input/resnet_lstm_tokenizer/pytorch/default/1/resnet_lstm_tokenizer.pth\"\n\nBNT_LARGE_CHECKPOINT = \"/kaggle/input/resnet50_transformer_vocab/pytorch/default/1/mscoco_transformer_checkpoint_best(2).pth\"\nBNT_LARGE_TURBO_CHECKPOINT = \"/kaggle/input/resnet_transformer_tokenizer/pytorch/default/1/resnet_transformer_tokenizer.pth\"\nBNT_LARGE_ULTRA_CHECKPOINT = \"/kaggle/input/inception_transformer_tokenizer/pytorch/default/1/inception_transformer_tokenizer.pth\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.085807Z","iopub.execute_input":"2025-08-25T17:59:16.086125Z","iopub.status.idle":"2025-08-25T17:59:16.090944Z","shell.execute_reply.started":"2025-08-25T17:59:16.086097Z","shell.execute_reply":"2025-08-25T17:59:16.090285Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\nEMBED_SIZE_LSTM = 512\nHIDDEN_SIZE_LSTM = 512\nNUM_LAYERS_LSTM = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.091507Z","iopub.execute_input":"2025-08-25T17:59:16.091748Z","iopub.status.idle":"2025-08-25T17:59:16.213879Z","shell.execute_reply.started":"2025-08-25T17:59:16.091725Z","shell.execute_reply":"2025-08-25T17:59:16.213172Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self, embed_size: int, cnn_type: str, train_CNN: bool = False):\n        super(EncoderCNN, self).__init__()\n        self.cnn_type = cnn_type.lower()\n        self.train_CNN = train_CNN\n\n        if \"tokenizer\" in self.cnn_type:\n            weights = ResNet50_Weights.DEFAULT\n            self.resnet = resnet50(weights=weights)\n            if not self.train_CNN:\n                for param in self.resnet.parameters():\n                    param.requires_grad = False\n            in_features_resnet = self.resnet.fc.in_features \n            self.resnet.fc = nn.Linear(in_features_resnet, embed_size)\n\n        elif \"resnet50\" in self.cnn_type:\n            resnet = models.resnet50(pretrained=True)\n            self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n            self.output_size = 2048\n            self.relu = nn.ReLU()\n            for p in self.resnet.parameters(): p.requires_grad = train_CNN\n\n        elif \"vgg16\" in self.cnn_type:\n            vgg = models.vgg16(pretrained=True)\n            self.vgg = vgg.features\n            self.output_size = 512\n            self.relu = nn.ReLU()\n            for p in self.vgg.parameters(): p.requires_grad = train_CNN\n\n        elif \"inception\" in self.cnn_type:\n            weights = Inception_V3_Weights.DEFAULT\n            self.inception = inception_v3(weights=weights, aux_logits=True)\n            self.inception.aux_logits = False\n            self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n            self.output_size = embed_size\n            self.relu = nn.ReLU()\n\n        if \"resnet\" in self.cnn_type or \"vgg\" in self.cnn_type:\n            self.linear = nn.Linear(self.output_size, embed_size)\n        # ortak aktivasyon + dropout\n        \n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, images: torch.Tensor) -> torch.Tensor:\n        if \"resnet\" in self.cnn_type:\n            x = self.resnet(images)      \n            x = x.view(x.size(0), -1)      \n            x = self.linear(x)             \n\n        elif \"inception\" in self.cnn_type:\n            x = self.inception(images)    \n        elif \"tokenizer\" in self.cnn_type:\n            features = self.resnet(images)\n            return self.dropout(features)\n        else:  # vgg\n            x = self.vgg(images)          \n            x = x.mean(dim=[2,3])         \n            x = self.linear(x)            \n\n        x = self.relu(x)\n        x = self.dropout(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.214748Z","iopub.execute_input":"2025-08-25T17:59:16.215045Z","iopub.status.idle":"2025-08-25T17:59:16.232060Z","shell.execute_reply.started":"2025-08-25T17:59:16.215004Z","shell.execute_reply":"2025-08-25T17:59:16.231342Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout_p=0.5):\n        super(DecoderRNN, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.embed_dropout = nn.Dropout(dropout_p)\n        self.lstm = nn.LSTM(\n            embed_size,\n            hidden_size,\n            num_layers,\n            dropout=dropout_p if num_layers > 1 else 0.0\n        )\n        self.output_dropout = nn.Dropout(dropout_p)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions, lengths=None):\n        embeddings = self.embed(captions)\n        embeddings = self.embed_dropout(embeddings)\n        embeddings = embeddings.permute(1, 0, 2)\n        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n\n        if lengths is not None:\n            packed_lengths = [l + 1 for l in lengths]\n            packed = pack_padded_sequence(embeddings, packed_lengths, enforce_sorted=False)\n            packed_outputs, _ = self.lstm(packed)\n            hiddens, _ = pad_packed_sequence(packed_outputs)\n        else:\n            hiddens, _ = self.lstm(embeddings)                                   \n\n        hiddens = self.output_dropout(hiddens)\n        outputs = self.linear(hiddens)                                             \n        outputs = outputs[1:]                                                     \n        outputs = outputs.permute(1, 0, 2)                                         \n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.232788Z","iopub.execute_input":"2025-08-25T17:59:16.232970Z","iopub.status.idle":"2025-08-25T17:59:16.252552Z","shell.execute_reply.started":"2025-08-25T17:59:16.232955Z","shell.execute_reply":"2025-08-25T17:59:16.251956Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, vocab, num_layers, cnn_type): # Changed default to resnet50\n        super(CNNtoRNN, self).__init__()\n        self.encoderCNN = EncoderCNN(embed_size, cnn_type) # Pass cnn_type\n        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n\n    def forward(self, images, captions, lengths=None):\n        features = self.encoderCNN(images)\n        outputs = self.decoderRNN(features, captions, lengths)\n        return outputs\n\n    def caption_image(self, image, vocab, max_length=50):\n        result_caption = []\n\n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n\n            for _ in range(max_length):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n\n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n\n        return [vocabulary.itos[idx] for idx in result_caption]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.253301Z","iopub.execute_input":"2025-08-25T17:59:16.253551Z","iopub.status.idle":"2025-08-25T17:59:16.275302Z","shell.execute_reply.started":"2025-08-25T17:59:16.253530Z","shell.execute_reply":"2025-08-25T17:59:16.274663Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class DecoderRNN_TOK(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, dropout_p=0.5):\n        super(DecoderRNN_TOK, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.embed_dropout = nn.Dropout(dropout_p)\n        self.lstm = nn.LSTM(\n            embed_size,\n            hidden_size,\n            num_layers,\n            dropout=dropout_p if num_layers > 1 else 0.0\n        )\n        self.output_dropout = nn.Dropout(dropout_p)\n        self.linear = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, features, captions, lengths=None): # lengths artık kullanılmayacak\n        embeddings = self.embed(captions)\n        embeddings = self.embed_dropout(embeddings)\n\n        embeddings = embeddings.permute(1, 0, 2)\n        embeddings = torch.cat(\n            (features.unsqueeze(0), embeddings), dim=0\n        )\n\n        hiddens, _ = self.lstm(embeddings)\n\n        hiddens = self.output_dropout(hiddens)\n        outputs = self.linear(hiddens)\n        outputs = outputs[1:]\n        outputs = outputs.permute(1, 0, 2)\n\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.275986Z","iopub.execute_input":"2025-08-25T17:59:16.276212Z","iopub.status.idle":"2025-08-25T17:59:16.295504Z","shell.execute_reply.started":"2025-08-25T17:59:16.276190Z","shell.execute_reply":"2025-08-25T17:59:16.294777Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class CNNtoRNN_TOK(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, vocab, num_layers, cnn_type=\"tokenizer\"): # vocab_size BERT'ten gelecek\n        super(CNNtoRNN_TOK, self).__init__()\n        self.encoderCNN = EncoderCNN(embed_size, cnn_type=\"tokenizer\") # EncoderCNN (ResNet50'li versiyon)\n        self.decoderRNN = DecoderRNN_TOK(embed_size, hidden_size, vocab_size, num_layers)\n\n    def forward(self, images, captions, lengths=None):\n        features = self.encoderCNN(images)\n        outputs = self.decoderRNN(features, captions, lengths)\n        return outputs\n\n    def caption_image(self, image_tensor: torch.Tensor, vocab: VocabularyT, max_length=50) -> list[str]:\n        result_ids = []\n        self.eval()\n\n        with torch.no_grad():\n            features = self.encoderCNN(image_tensor)\n            states = None\n            x = features.unsqueeze(0)\n            _, states = self.decoderRNN.lstm(x, states)\n            sos_id_tensor = torch.tensor([vocab.sos_token_id], device=features.device)\n            x = self.decoderRNN.embed(sos_id_tensor).unsqueeze(0) # Shape: (1, 1, embed_size)\n\n            for _ in range(max_length):\n                hiddens, states = self.decoderRNN.lstm(x, states) # hiddens: (1, 1, hidden_size)\n                \n                output_logits = self.decoderRNN.linear(hiddens.squeeze(0)) # (1, vocab_size)\n                \n                predicted_id = output_logits.argmax(1).item()\n                if predicted_id == vocab.eos_token_id:\n                    break\n                \n                result_ids.append(predicted_id)\n                \n                next_word_tensor = torch.tensor([predicted_id], device=features.device)\n                x = self.decoderRNN.embed(next_word_tensor).unsqueeze(0)\n\n        caption_tokens = [vocab.itos[idx] for idx in result_ids]\n        return caption_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.296285Z","iopub.execute_input":"2025-08-25T17:59:16.296506Z","iopub.status.idle":"2025-08-25T17:59:16.315011Z","shell.execute_reply.started":"2025-08-25T17:59:16.296483Z","shell.execute_reply":"2025-08-25T17:59:16.314233Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class CNNtoRNN_REP(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, model_type=\"resnet50\"): # Changed default to resnet50\n        super(CNNtoRNN_REP, self).__init__()\n        self.encoderCNN = EncoderCNN(embed_size, model_type) # Pass cnn_type\n        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n\n    def forward(self, images, captions, lengths=None):\n        features = self.encoderCNN(images)\n        outputs = self.decoderRNN(features, captions, lengths)\n        return outputs\n\n    def caption_image(self, image, vocabulary=rep_vocabulary, max_length=50):\n        result_caption = []\n\n        with torch.no_grad():\n            x = self.encoderCNN(image).unsqueeze(0)\n            states = None\n\n            for _ in range(max_length):\n                hiddens, states = self.decoderRNN.lstm(x, states)\n                output = self.decoderRNN.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                result_caption.append(predicted.item())\n                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n\n                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n                    break\n\n        return [vocabulary.itos[idx] for idx in result_caption]\n\n    def decoder_step(self, input_token, hidden_state):\n        emb = self.decoderRNN.embed(input_token)  \n        emb = emb.unsqueeze(1)                    \n\n        output, new_hidden = self.decoderRNN.lstm(emb, hidden_state)\n        logits = self.decoderRNN.linear(output.squeeze(0))  \n        return logits, new_hidden\n\n    def caption_image_beam_search(\n        self,\n        image_tensor,         \n        vocabulary=rep_vocabulary,\n        beam_size=5,\n        max_length=20,\n        repetition_penalty=1.2,\n        ngram_block_size=3,\n        length_penalty=1.0\n    ):\n        device = next(self.parameters()).device\n        self.eval()\n\n        with torch.no_grad():\n            features = self.encoderCNN(image_tensor.to(device))  \n\n        vocab_stoi = vocabulary.stoi\n        vocab_itos = vocabulary.itos\n        start_token_id = vocab_stoi[\"<SOS>\"]\n        eos_token_id   = vocab_stoi[\"<EOS>\"]\n        vocab_size     = len(vocabulary)\n\n        beams = []\n        beams.append({\n            'sequence': [start_token_id],\n            'score': 0.0,\n            'normalized_score': 0.0,\n            'hidden': None, \n            'cell': None\n        })\n        completed_beams = []\n\n        for step in range(max_length):\n            all_candidates = []\n\n            for beam in beams:\n                last_token = beam['sequence'][-1]\n\n                if last_token == eos_token_id:\n                    completed_beams.append(beam)\n                    continue\n\n                curr_token_tensor = torch.tensor([last_token], device=device)\n\n                hidden_state = None\n                if beam['hidden'] is not None:\n                    hidden_state = (beam['hidden'], beam['cell'])\n\n                with torch.no_grad():\n                    if step == 0:\n                        x0 = features.unsqueeze(0)  # (1, 1, embed_size)\n                        output, new_hidden = self.decoderRNN.lstm(x0, None)\n                        logits = self.decoderRNN.linear(output.squeeze(0))  \n                    else:\n                        logits, new_hidden = self.decoder_step(curr_token_tensor, hidden_state)\n\n                logits = logits.squeeze(0)  \n\n                if len(beam['sequence']) > 0:\n                    token_counts = {}\n                    for tkn in beam['sequence']:\n                        token_counts[tkn] = token_counts.get(tkn, 0) + 1\n                    for tkn, cnt in token_counts.items():\n                        if tkn < vocab_size and cnt > 0:\n                            logits[tkn] = logits[tkn] / (repetition_penalty ** cnt)\n\n\n                if len(beam['sequence']) >= ngram_block_size - 1:\n                    prev_ngrams = set()\n                    seq = beam['sequence']\n                    n = ngram_block_size - 1\n                    for i in range(len(seq) - n + 1):\n                        prev_ngrams.add(tuple(seq[i:i+n]))\n                    blocked = set()\n                    for ngram in prev_ngrams:\n                        for candidate_id in range(vocab_size):\n                            new_ngram = ngram + (candidate_id,)\n                            if new_ngram in set(tuple(seq[i:i+ngram_block_size]) \n                                               for i in range(len(seq) - ngram_block_size + 1)):\n                                blocked.add(candidate_id)\n                    for bt in blocked:\n                        logits[bt] = -float('inf')\n\n                probs = F.softmax(logits, dim=-1)   \n                top_probs, top_indices = torch.topk(probs, beam_size)\n\n                for i in range(beam_size):\n                    token_id = top_indices[i].item()\n                    prob = top_probs[i].item()\n\n                    new_seq = beam['sequence'] + [token_id]\n                    new_score = beam['score'] + np.log(prob + 1e-8)\n                    length_norm = (len(new_seq) ** length_penalty)\n                    new_norm_score = new_score / length_norm\n\n                    cand = {\n                        'sequence': new_seq,\n                        'score': new_score,\n                        'normalized_score': new_norm_score,\n                        'hidden': new_hidden[0],  \n                        'cell': new_hidden[1]     \n                    }\n                    all_candidates.append(cand)\n\n            all_candidates.sort(key=lambda x: x['normalized_score'], reverse=True)\n            beams = []\n            for cand in all_candidates:\n                if len(beams) >= beam_size:\n                    break\n                if cand['sequence'][-1] == eos_token_id:\n                    completed_beams.append(cand)\n                else:\n                    beams.append(cand)\n\n            if len(beams) == 0:\n                break\n\n        if len(completed_beams) == 0:\n            completed_beams = beams\n\n        best_beam = max(completed_beams, key=lambda x: x['normalized_score'])\n        seq_ids = best_beam['sequence']  \n\n        if seq_ids[0] == start_token_id:\n            seq_ids = seq_ids[1:]\n        if eos_token_id in seq_ids:\n            seq_ids = seq_ids[:seq_ids.index(eos_token_id)]\n        caption_words = [vocab_itos[idx] if idx in vocab_itos else \"<UNK>\" for idx in seq_ids]\n        return caption_words\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T17:59:16.315810Z","iopub.execute_input":"2025-08-25T17:59:16.316099Z","iopub.status.idle":"2025-08-25T17:59:16.335138Z","shell.execute_reply.started":"2025-08-25T17:59:16.316073Z","shell.execute_reply":"2025-08-25T17:59:16.334376Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## transformers","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport statistics\nimport torchvision.models as models\nfrom torchvision.models import resnet50, ResNet50_Weights, inception_v3\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport math\nfrom torchvision.models.feature_extraction import create_feature_extractor\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1) \n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass ImageFeatureExtractor(nn.Module):\n    def __init__(self, d_model, cnn_type, train_CNN=False):\n        super(ImageFeatureExtractor, self).__init__()\n        self.train_CNN = train_CNN\n        if \"inception\" in cnn_type:\n            weights = Inception_V3_Weights.DEFAULT\n            inception_model_full = inception_v3(weights=weights) \n            inception_model_full.aux_logits = False \n\n            return_nodes = {'Mixed_7c': 'spatial_features'} \n            self.feature_extractor_backbone = create_feature_extractor(inception_model_full, return_nodes=return_nodes)\n        else:\n            weights = ResNet50_Weights.DEFAULT\n            resnet_model = resnet50(weights=weights)\n\n            return_nodes = {'layer4': 'spatial_features'}\n            self.feature_extractor_backbone = create_feature_extractor(\n                resnet_model,\n                return_nodes=return_nodes\n            )\n\n        self.feature_proj = nn.Conv2d(2048, d_model, kernel_size=1)\n\n        if not self.train_CNN:\n            for param in self.feature_extractor_backbone.parameters():\n                param.requires_grad = False\n\n    def forward(self, images):\n        extracted_outputs = self.feature_extractor_backbone(images)\n        spatial_features = extracted_outputs['spatial_features']\n        projected_features = self.feature_proj(spatial_features)\n\n        batch_size, d_model_channels, h_out, w_out = projected_features.shape\n        features_seq = projected_features.view(batch_size, d_model_channels, h_out * w_out)\n        features_seq = features_seq.permute(0, 2, 1)\n\n        return features_seq \n\nclass ImageTransformerEncoder(nn.Module):\n    def __init__(self, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n        super(ImageTransformerEncoder, self).__init__()\n        self.d_model = d_model\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        \n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n                                                   dim_feedforward=dim_feedforward, dropout=dropout, \n                                                   batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n\n    def forward(self, src_img_features, src_padding_mask=None):\n        src_img_features_permuted = src_img_features.permute(1, 0, 2)\n        src_with_pe = self.pos_encoder(src_img_features_permuted)\n        src_with_pe_batch_first = src_with_pe.permute(1, 0, 2)\n        memory = self.transformer_encoder(src_with_pe_batch_first, src_key_padding_mask=src_padding_mask)     \n        return memory\n\nclass TextTransformerDecoder(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, num_decoder_layers, dim_feedforward, dropout=0.1, max_seq_len=100):\n        super(TextTransformerDecoder, self).__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len) \n        \n        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead,\n                                                   dim_feedforward=dim_feedforward, dropout=dropout,\n                                                   batch_first=True) \n        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        self.fc_out = nn.Linear(d_model, vocab_size)\n\n    def forward(self, tgt_caption_tokens, memory, tgt_mask=None, \n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        \n        tgt_emb = self.embedding(tgt_caption_tokens) * math.sqrt(self.d_model) \n\n        tgt_emb_permuted = tgt_emb.permute(1,0,2) \n        tgt_with_pe = self.pos_encoder(tgt_emb_permuted)\n        tgt_with_pe_batch_first = tgt_with_pe.permute(1,0,2) \n        output = self.transformer_decoder(tgt=tgt_with_pe_batch_first, \n                                           memory=memory, \n                                           tgt_mask=tgt_mask,\n                                           tgt_key_padding_mask=tgt_key_padding_mask,\n                                           memory_key_padding_mask=memory_key_padding_mask)\n        \n        return self.fc_out(output)\n\n    def generate_square_subsequent_mask(self, sz):\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\nclass CaptioningTransformer(nn.Module):\n    def __init__(self, num_tokens_vocab, d_model, nhead, num_encoder_layers, num_decoder_layers, \n                 dim_feedforward, dropout=0.1, max_seq_len=100, cnn_type=\"resnet50\"):\n        super(CaptioningTransformer, self).__init__()\n        \n        self.image_feature_extractor = ImageFeatureExtractor(d_model, cnn_type) \n        self.image_transformer_encoder = ImageTransformerEncoder(d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n        self.text_transformer_decoder = TextTransformerDecoder(num_tokens_vocab, d_model, nhead, num_decoder_layers, \n                                                               dim_feedforward, dropout, max_seq_len)\n        self.d_model = d_model\n        self.num_tokens_vocab = num_tokens_vocab\n\n    def _generate_padding_mask(self, sequences, pad_token_id):\n        return (sequences == pad_token_id)\n\n    def forward(self, images, captions_input, captions_target_padding_mask=None):\n        img_spatial_features_seq = self.image_feature_extractor(images, cnn_type)\n        image_memory = self.image_transformer_encoder(img_spatial_features_seq, src_padding_mask=None)\n        tgt_seq_len = captions_input.size(1)\n        device = captions_input.device\n        tgt_mask = self.text_transformer_decoder.generate_square_subsequent_mask(tgt_seq_len).to(device)\n        decoder_output_logits = self.text_transformer_decoder(\n            tgt_caption_tokens=captions_input, \n            memory=image_memory, \n            tgt_mask=tgt_mask,\n            tgt_key_padding_mask=captions_target_padding_mask,\n            memory_key_padding_mask=None\n        )\n        return decoder_output_logits\n\n    @torch.no_grad()\n    def generate_caption(self, image_tensor, vocabulary, start_token_id, end_token_id, pad_token_id, max_length=50):\n        self.eval()\n        device = image_tensor.device\n\n        img_spatial_features_seq = self.image_feature_extractor(image_tensor)\n        image_memory = self.image_transformer_encoder(img_spatial_features_seq)\n        generated_ids = torch.tensor([[start_token_id]], dtype=torch.long, device=device)\n\n        for _ in range(max_length - 1):\n            tgt_seq_len = generated_ids.size(1)\n            tgt_mask = self.text_transformer_decoder.generate_square_subsequent_mask(tgt_seq_len).to(device)\n            \n            tgt_padding_mask = self._generate_padding_mask(generated_ids, pad_token_id).to(device)\n            output_logits = self.text_transformer_decoder(\n                tgt_caption_tokens=generated_ids, \n                memory=image_memory, \n                tgt_mask=tgt_mask,\n                tgt_key_padding_mask=tgt_padding_mask,\n                memory_key_padding_mask=None\n            )\n            next_token_logits = output_logits[:, -1, :] \n            next_token_id = torch.argmax(next_token_logits, dim=-1) \n            generated_ids = torch.cat((generated_ids, next_token_id.unsqueeze(1)), dim=1)\n\n            if next_token_id.item() == end_token_id:\n                break\n        \n        output_tokens = [vocabulary.itos[idx.item()] for idx in generated_ids[0, 1:]]\n        return output_tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:10.882818Z","iopub.execute_input":"2025-08-25T18:00:10.883236Z","iopub.status.idle":"2025-08-25T18:00:10.909626Z","shell.execute_reply.started":"2025-08-25T18:00:10.883210Z","shell.execute_reply":"2025-08-25T18:00:10.909047Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class CaptioningTransformer_T(nn.Module):\n    def __init__(self, num_tokens_vocab: int, d_model: int, nhead: int, num_encoder_layers: int, num_decoder_layers: int,\n                 dim_feedforward: int, pad_token_id_for_masking: int, dropout: float = 0.1, max_seq_len: int = 100, cnn_type=\"resnet50\"):\n        super(CaptioningTransformer_T, self).__init__()\n        self.image_feature_extractor = ImageFeatureExtractor(d_model, cnn_type=cnn_type)\n        self.image_transformer_encoder = ImageTransformerEncoder(d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n        self.text_transformer_decoder = TextTransformerDecoder(num_tokens_vocab, d_model, nhead, num_decoder_layers,\n                                                               dim_feedforward, dropout, max_seq_len)\n        self.d_model = d_model\n        self.num_tokens_vocab = num_tokens_vocab\n        self.pad_token_id_for_masking = pad_token_id_for_masking \n\n    def _generate_padding_mask(self, sequences: torch.Tensor) -> torch.Tensor:\n        return (sequences == self.pad_token_id_for_masking) \n\n    def forward(self, images: torch.Tensor, captions_input: torch.Tensor, captions_target_padding_mask: torch.Tensor = None):\n        img_spatial_features_seq = self.image_feature_extractor(images)\n        image_memory = self.image_transformer_encoder(img_spatial_features_seq, src_padding_mask=None)\n\n        tgt_seq_len = captions_input.size(1)\n        device = captions_input.device\n        tgt_mask = self.text_transformer_decoder.generate_square_subsequent_mask(tgt_seq_len).to(device)\n\n        if captions_target_padding_mask is None:\n            captions_target_padding_mask = self._generate_padding_mask(captions_input).to(device)\n\n        decoder_output_logits = self.text_transformer_decoder(\n            tgt_caption_tokens=captions_input,\n            memory=image_memory,\n            tgt_mask=tgt_mask, # Causal mask\n            tgt_key_padding_mask=captions_target_padding_mask,\n            memory_key_padding_mask=None \n        )\n        return decoder_output_logits\n    @torch.no_grad()\n    def generate_caption(self, image_tensor: torch.Tensor, vocabulary_tokenizer: VocabularyT, max_length: int = 50) -> str:\n        self.eval()\n        device = image_tensor.device\n        img_spatial_features_seq = self.image_feature_extractor(image_tensor)\n        image_memory = self.image_transformer_encoder(img_spatial_features_seq)\n        generated_ids = torch.tensor([[vocabulary_tokenizer.sos_token_id]], dtype=torch.long, device=device)\n\n        for _ in range(max_length - 1):\n            tgt_seq_len = generated_ids.size(1)\n            tgt_mask = self.text_transformer_decoder.generate_square_subsequent_mask(tgt_seq_len).to(device)\n            tgt_padding_mask = self._generate_padding_mask(generated_ids).to(device)\n            output_logits = self.text_transformer_decoder(\n                tgt_caption_tokens=generated_ids,\n                memory=image_memory,\n                tgt_mask=tgt_mask,\n                tgt_key_padding_mask=tgt_padding_mask\n            )\n            next_token_logits = output_logits[:, -1, :]\n            \n            next_token_id = torch.argmax(next_token_logits, dim=-1)\n\n            generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(1)], dim=1)\n            if next_token_id.item() == vocabulary_tokenizer.eos_token_id:\n                break\n        \n        caption = vocabulary_tokenizer.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        \n        self.train() \n        \n        return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:10.982649Z","iopub.execute_input":"2025-08-25T18:00:10.983629Z","iopub.status.idle":"2025-08-25T18:00:10.993178Z","shell.execute_reply.started":"2025-08-25T18:00:10.983605Z","shell.execute_reply":"2025-08-25T18:00:10.992496Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def get_tokenizer_model_for_inference(checkpoint_path, vocab, cnn_type, device):\n    checkpoint = torch.load(checkpoint_path, map_location=device)\n    if \"config\" in checkpoint:\n        cfg = checkpoint[\"config\"]\n        print(\"[INFO] Checkpoint'ten config yüklendi.\")\n    else:\n        print(\"[UYARI] Config bilgisi bulunamadı. Varsayılan yapılandırma kullanılıyor.\")\n        cfg = {\n            \"d_model\": 256,\n            \"nhead\": 8,\n            \"num_encoder_layers\": 3,\n            \"num_decoder_layers\": 3,\n            \"dim_feedforward\": 1024,\n            \"transformer_dropout\": 0.1,\n            \"vocab_size\": len(vocab),\n            \"max_caption_len\": 100\n        }\n    cfg[\"vocab_size\"] = len(vocab)\n    pad_token_id = vocab.pad_token_id\n    model = CaptioningTransformer_T(\n        num_tokens_vocab=cfg[\"vocab_size\"],\n        d_model=cfg[\"d_model\"],\n        nhead=cfg[\"nhead\"],\n        num_encoder_layers=cfg[\"num_encoder_layers\"],\n        num_decoder_layers=cfg[\"num_decoder_layers\"],\n        dim_feedforward=cfg[\"dim_feedforward\"],\n        dropout=cfg[\"transformer_dropout\"],\n        max_seq_len=cfg[\"max_caption_len\"],\n        pad_token_id_for_masking=pad_token_id,\n        cnn_type=cnn_type\n    ).to(device)\n\n    state_dict = checkpoint.get(\"state_dict\", checkpoint)\n    model.load_state_dict(state_dict)\n    print(\"[✓] Model ağırlıkları başarıyla yüklendi.\")\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:10.994254Z","iopub.execute_input":"2025-08-25T18:00:10.994471Z","iopub.status.idle":"2025-08-25T18:00:11.012334Z","shell.execute_reply.started":"2025-08-25T18:00:10.994455Z","shell.execute_reply":"2025-08-25T18:00:11.011699Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def generate_caption_from_image_path_T(image_path, model, tokenizer, device):\n    weights = ResNet50_Weights.DEFAULT\n    preprocess = weights.transforms()\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = preprocess(image).unsqueeze(0).to(device)\n\n    model.eval()\n    with torch.no_grad():\n        caption = model.generate_caption(\n            image_tensor=image_tensor,\n            vocabulary_tokenizer=tokenizer,\n            max_length=50\n        )\n    return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:11.013014Z","iopub.execute_input":"2025-08-25T18:00:11.013235Z","iopub.status.idle":"2025-08-25T18:00:11.033768Z","shell.execute_reply.started":"2025-08-25T18:00:11.013220Z","shell.execute_reply":"2025-08-25T18:00:11.033168Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def get_transformer_model_for_inference(vocab_size, checkpoint_path, model_type):\n    print(f\"Checkpoint yükleniyor: {checkpoint_path}\")\n    try:\n        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    except FileNotFoundError:\n        print(f\"HATA: Checkpoint dosyası bulunamadı: {checkpoint_path}\")\n        return None, None\n    except Exception as e:\n        print(f\"HATA: Checkpoint yüklenirken bir sorun oluştu (torch.load): {e}\")\n        return None, None\n\n    d_model = checkpoint.get('d_model', 256)\n    nhead = checkpoint.get('nhead', 8)\n    num_encoder_layers = checkpoint.get('num_encoder_layers', 3)\n    num_decoder_layers = checkpoint.get('num_decoder_layers', 3)\n    dim_feedforward = checkpoint.get('dim_feedforward', 1024)\n    transformer_dropout = checkpoint.get('transformer_dropout', 0.1)\n    actual_vocab_size = checkpoint.get('vocab_size', vocab_size) \n\n\n    print(\"Transformer modeli oluşturuluyor...\")\n    print(f\"Kullanılan parametreler: d_model={d_model}, nhead={nhead}, enc_layers={num_encoder_layers}, dec_layers={num_decoder_layers}, ffw_dim={dim_feedforward}, vocab_size={actual_vocab_size}\")\n\n    model = CaptioningTransformer(\n        num_tokens_vocab=actual_vocab_size,\n        d_model=d_model,\n        nhead=nhead,\n        num_encoder_layers=num_encoder_layers,\n        num_decoder_layers=num_decoder_layers,\n        dim_feedforward=dim_feedforward,\n        dropout=transformer_dropout,\n        max_seq_len=100,\n        cnn_type=model_type\n    ).to(DEVICE)\n    \n    try:\n        model.load_state_dict(checkpoint[\"state_dict\"])\n    except Exception as e:\n        print(f\"HATA: Model state_dict yüklenirken sorun oluştu: {e}\")\n        print(\"Model mimarisi ile checkpoint'teki ağırlıklar uyumsuz olabilir.\")\n        return None, vocab \n        \n    model.eval()\n    print(\"Model başarıyla yüklendi ve evaluation moduna alındı.\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:11.034619Z","iopub.execute_input":"2025-08-25T18:00:11.034857Z","iopub.status.idle":"2025-08-25T18:00:11.051689Z","shell.execute_reply.started":"2025-08-25T18:00:11.034840Z","shell.execute_reply":"2025-08-25T18:00:11.050988Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def get_model_for_inference(vocab_size, vocab, checkpoint_path, model_type, isTokenizer=False):\n    print(\"Model oluşturuluyor...\")\n    if \"REP\" in model_type:\n        model = CNNtoRNN_REP(EMBED_SIZE_LSTM, HIDDEN_SIZE_LSTM, vocab_size, NUM_LAYERS_LSTM, model_type).to(DEVICE)\n    elif \"inception\" in model_type:\n        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n        embed_size  = checkpoint['embed_size']\n        hidden_size = checkpoint['hidden_size']\n        num_layers  = checkpoint['num_layers']\n        vocab_size  = checkpoint['vocab_size']\n        model = CNNtoRNN(\n        embed_size  = embed_size,\n        hidden_size = hidden_size,\n        vocab_size  = vocab_size,\n        vocab = vocab,\n        num_layers  = num_layers,\n        cnn_type    = model_type      \n        ).to(DEVICE)\n        sd = checkpoint['state_dict']\n        model.load_state_dict(sd)\n    else:\n        if isTokenizer:\n            model = CNNtoRNN_TOK(768, 768, len(vocabT), vocabT, NUM_LAYERS_LSTM, model_type).to(DEVICE)\n        else:\n            model = CNNtoRNN(EMBED_SIZE_LSTM, HIDDEN_SIZE_LSTM, vocab_size, vocab, NUM_LAYERS_LSTM, model_type).to(DEVICE)\n    \n    print(f\"Checkpoint yükleniyor: {checkpoint_path}\")\n    try:\n        if \"inception\" not in model_type:\n           checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n           sd = checkpoint[\"state_dict\"]  \n           model.load_state_dict(sd)\n            \n    except FileNotFoundError:\n        print(f\"HATA: Checkpoint dosyası bulunamadı: {checkpoint_path}\")\n        return None\n    except Exception as e:\n        print(f\"HATA: Checkpoint yüklenirken bir sorun oluştu: {e}\")\n        return None\n        \n    model.eval()\n    print(\"Model başarıyla yüklendi ve evaluation moduna alındı.\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:11.053594Z","iopub.execute_input":"2025-08-25T18:00:11.053770Z","iopub.status.idle":"2025-08-25T18:00:11.073134Z","shell.execute_reply.started":"2025-08-25T18:00:11.053757Z","shell.execute_reply":"2025-08-25T18:00:11.072377Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# GET MODELS\nif not vocabulary or len(vocabulary) <= 4:\n    print(\"Kelime dağarcığı oluşturulamadı veya çok küçük. Test işlemi yapılamıyor.\")\nelse:\n    VOCAB_SIZE = len(vocabulary)\n    model_bnl_mini = get_model_for_inference(VOCAB_SIZE, vocabulary, BNL_MINI_CHECKPOINT, \"inceptionv3\")\n    model_bnl_large = get_model_for_inference(VOCAB_SIZE, vocabulary, BNL_LARGE_CHECKPOINT, \"resnet50\")\n    model_bnl_medium = get_model_for_inference(VOCAB_SIZE, vocabulary, BNL_MID_CHECKPOINT, \"vgg16\")\n    model_bnl_large_rep = get_model_for_inference(len(rep_vocabulary), vocabulary, BNL_LARGE_REP_CHECKPOINT, \"resnet50-REP\")\n    model_bnl_large_turbo = get_model_for_inference(len(vocabT), vocabT, BNL_LARGE_TURBO_CHECKPOINT, \"tokenizer\", isTokenizer=True)\n    \n    model_bnt_large = get_transformer_model_for_inference(VOCAB_SIZE, BNT_LARGE_CHECKPOINT, \"resnet50\")\n    model_bnt_large_turbo = get_tokenizer_model_for_inference(BNT_LARGE_TURBO_CHECKPOINT, vocabT, \"resnet50\", DEVICE)\n    model_bnt_large_ultra = get_tokenizer_model_for_inference(BNT_LARGE_ULTRA_CHECKPOINT, vocabT, \"inceptionv3\", DEVICE)\n        \n\nmodels_dict = {\n    \"BNL-Mini\": (model_bnl_mini, \"inceptionv3\"),\n    \"BNL-Medium\": (model_bnl_medium, \"vgg168\"),\n    \"BNL-Large\": (model_bnl_large, \"resnet50\"),\n    \"BNL-Large-REP\": (model_bnl_large_rep, \"resnet50\"),\n    \"BNL-Large-Turbo\": (model_bnl_large_turbo, \"resnet50\"),\n    \"BNT-Large\": (model_bnt_large, \"resnet50\"),\n    \"BNT-Large-Turbo\": (model_bnt_large_turbo, \"resnet50\"),\n    \"BNT-Large-Ultra\": (model_bnt_large_ultra, \"inceptionv3\")\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:11.073779Z","iopub.execute_input":"2025-08-25T18:00:11.073945Z","iopub.status.idle":"2025-08-25T18:00:47.026762Z","shell.execute_reply.started":"2025-08-25T18:00:11.073932Z","shell.execute_reply":"2025-08-25T18:00:47.025904Z"}},"outputs":[{"name":"stdout","text":"Model oluşturuluyor...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 175MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Checkpoint yükleniyor: /kaggle/input/inception_lstm_vocab/pytorch/default/1/inception_lstm_vocab.pth\nModel başarıyla yüklendi ve evaluation moduna alındı.\nModel oluşturuluyor...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 178MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Checkpoint yükleniyor: /kaggle/input/resnet50_lstm_combined/pytorch/default/1/resnet50_combined.pth\nModel başarıyla yüklendi ve evaluation moduna alındı.\nModel oluşturuluyor...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 207MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Checkpoint yükleniyor: /kaggle/input/vgg16_lstm_combined/pytorch/default/1/vgg16_combined_model.pth\nModel başarıyla yüklendi ve evaluation moduna alındı.\nModel oluşturuluyor...\nCheckpoint yükleniyor: /kaggle/input/rep_penalty/pytorch/default/1/mscoco_checkpoint_best(7).pth\nModel başarıyla yüklendi ve evaluation moduna alındı.\nModel oluşturuluyor...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 203MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Checkpoint yükleniyor: /kaggle/input/resnet_lstm_tokenizer/pytorch/default/1/resnet_lstm_tokenizer.pth\nModel başarıyla yüklendi ve evaluation moduna alındı.\nCheckpoint yükleniyor: /kaggle/input/resnet50_transformer_vocab/pytorch/default/1/mscoco_transformer_checkpoint_best(2).pth\nTransformer modeli oluşturuluyor...\nKullanılan parametreler: d_model=256, nhead=8, enc_layers=3, dec_layers=3, ffw_dim=1024, vocab_size=23233\nModel başarıyla yüklendi ve evaluation moduna alındı.\n[INFO] Checkpoint'ten config yüklendi.\n[✓] Model ağırlıkları başarıyla yüklendi.\n[INFO] Checkpoint'ten config yüklendi.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/feature_extraction.py:175: UserWarning: NOTE: The nodes obtained by tracing the model in eval mode are a subsequence of those obtained in train mode. When choosing nodes for feature extraction, you may need to specify output nodes for train and eval mode separately.\n  warnings.warn(msg + suggestion_msg)\n","output_type":"stream"},{"name":"stdout","text":"[✓] Model ağırlıkları başarıyla yüklendi.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## before-inference methods","metadata":{}},{"cell_type":"code","source":"def transform_image_for_inference(image_path, model_type):\n    if model_type == \"resnet50\":\n        weights = ResNet50_Weights.DEFAULT\n    elif model_type == \"vgg16\":\n         weights = VGG16_Weights.DEFAULT\n    else:\n         weights = Inception_V3_Weights.DEFAULT\n\n    preprocessed = weights.transforms(antialias=True) # Bu, resize, crop, ToTensor, Normalize içerir\n    \n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        transformed_image = preprocessed(image)\n        return transformed_image.unsqueeze(0) # Batch boyutu ekle (1, C, H, W)\n    except FileNotFoundError:\n        print(f\"HATA: Görüntü dosyası bulunamadı: {image_path}\")\n        return None\n    except Exception as e:\n        print(f\"HATA: Görüntü işlenirken bir sorun oluştu: {e}\")\n        return None\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:47.027635Z","iopub.execute_input":"2025-08-25T18:00:47.027933Z","iopub.status.idle":"2025-08-25T18:00:47.033306Z","shell.execute_reply.started":"2025-08-25T18:00:47.027906Z","shell.execute_reply":"2025-08-25T18:00:47.032595Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def generate_caption_from_image_path(image_path_to_test, model, model_type, vocab):\n    if model is None or vocab is None:\n        print(\"Model veya kelime dağarcığı yüklenemedi. İşlem iptal edildi.\")\n        return\n\n    print(f\"\\nTest edilecek görüntü: {image_path_to_test}\")\n    image_tensor = transform_image_for_inference(image_path_to_test, model_type)\n\n    if image_tensor is None:\n        return\n\n    image_tensor = image_tensor.to(DEVICE)\n    \n    print(\"Altyazı üretiliyor...\")\n    \n    predicted_caption_tokens = model.caption_image(image_tensor, vocab, max_length=50)\n\n    try:\n        eos_index = predicted_caption_tokens.index(\"<EOS>\")\n        predicted_caption_tokens = predicted_caption_tokens[:eos_index]\n    except ValueError:\n        pass \n\n    generated_caption = \" \".join(predicted_caption_tokens)\n    print(f\"Üretilen Altyazı: {generated_caption}\")\n    return generated_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:47.034086Z","iopub.execute_input":"2025-08-25T18:00:47.034887Z","iopub.status.idle":"2025-08-25T18:00:47.054994Z","shell.execute_reply.started":"2025-08-25T18:00:47.034868Z","shell.execute_reply":"2025-08-25T18:00:47.054392Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def generate_caption_for_image_path_transformers(image_path_to_test, model, model_type, vocab):\n    if model is None or vocab is None:\n        print(\"Model veya kelime dağarcığı yüklenemedi. İşlem iptal edildi.\")\n        return None\n\n    print(f\"\\nTest edilecek görüntü: {image_path_to_test}\")\n    image_tensor = transform_image_for_inference(image_path_to_test, model_type)\n\n    if image_tensor is None:\n        return None\n\n    image_tensor = image_tensor.to(DEVICE)\n    \n    print(\"Altyazı üretiliyor (Transformer ile)...\")\n    \n    try:\n        sos_token_id = vocab.stoi[\"<SOS>\"]\n        eos_token_id = vocab.stoi[\"<EOS>\"]\n        pad_token_id = vocab.stoi[\"<PAD>\"]\n    except AttributeError:\n        print(\"HATA: Kelime dağarcığı nesnesinde 'stoi' özelliği bulunamadı. Lütfen Vocabulary sınıfınızı veya reconstruct_vocab_from_checkpoint_data fonksiyonunu kontrol edin.\")\n        return None\n    except KeyError as e:\n        print(f\"HATA: Özel token ({e}) kelime dağarcığında bulunamadı.\")\n        return None\n\n    predicted_caption_tokens = model.generate_caption(\n        image_tensor, \n        vocab,\n        start_token_id=sos_token_id,\n        end_token_id=eos_token_id,\n        pad_token_id=pad_token_id,\n        max_length=50\n    )\n\n    try:\n      eos_index = predicted_caption_tokens.index(\"<EOS>\")\n      predicted_caption_tokens = predicted_caption_tokens[:eos_index]\n    except ValueError:\n        pass \n    generated_caption = \" \".join(predicted_caption_tokens)\n    print(f\"Üretilen Altyazı: {generated_caption}\")\n    return generated_caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:47.055855Z","iopub.execute_input":"2025-08-25T18:00:47.056534Z","iopub.status.idle":"2025-08-25T18:00:47.072914Z","shell.execute_reply.started":"2025-08-25T18:00:47.056509Z","shell.execute_reply":"2025-08-25T18:00:47.072267Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## gradio fn","metadata":{}},{"cell_type":"code","source":"def gradio_generate_caption(model_choice, image_path):\n    selected_model, model_type = models_dict.get(model_choice, (None, None))\n    if selected_model is None:\n        return f\"⚠️ Geçersiz model seçimi: {model_choice}\"\n\n    if selected_model is model_bnl_large_rep:\n        caption = generate_caption_from_image_path(image_path, selected_model, model_type, rep_vocabulary)\n    elif selected_model is model_bnt_large:\n        caption = generate_caption_for_image_path_transformers(image_path, selected_model, model_type, vocabulary)\n    elif selected_model is model_bnt_large_turbo or selected_model is model_bnt_large_ultra:\n        caption = generate_caption_from_image_path_T(image_path, selected_model, vocabT, DEVICE)\n    elif selected_model is model_bnl_large_turbo:\n        caption = generate_caption_from_image_path(image_path, selected_model, model_type, vocabT)\n    else:\n        caption = generate_caption_from_image_path(image_path, selected_model, model_type, vocabulary)\n        \n    if not caption:\n        return \"Altyazı üretilemedi ya da boş döndü.\"\n    return caption\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:47.073622Z","iopub.execute_input":"2025-08-25T18:00:47.073831Z","iopub.status.idle":"2025-08-25T18:00:47.091335Z","shell.execute_reply.started":"2025-08-25T18:00:47.073805Z","shell.execute_reply":"2025-08-25T18:00:47.090587Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## ngrok - server","metadata":{}},{"cell_type":"code","source":"ngrok.kill()\ndemo = gr.Interface(\n    fn=gradio_generate_caption,\n    inputs=[\n        gr.Dropdown(choices=list(models_dict.keys()),   \n                    value=\"BNL-Large\",                   \n                    label=\"Choose a model\"),\n        gr.Image(type=\"filepath\", label=\"Upload a picture\")\n    ],\n    outputs=gr.Textbox(label=\"Produced caption\"),\n    title=\"Automatic Title Extraction Using Deep Learning\",\n    description=\"\"\"\n    Upload a picture to get it captioned.\n    \"\"\",\n    allow_flagging=\"never\"\n)\n\npublic_url = ngrok.connect(addr=\"7860\")\nprint(\"🔗 ngrok public URL:\", public_url)\n\ndef run_gradio():\n    demo.launch(\n        server_name=\"0.0.0.0\",\n        server_port=7860,\n        share=False\n    )\n\nthread = threading.Thread(target=run_gradio, daemon=True)\nthread.start()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-25T18:00:47.681125Z","iopub.execute_input":"2025-08-25T18:00:47.681439Z","iopub.status.idle":"2025-08-25T18:00:48.325496Z","shell.execute_reply.started":"2025-08-25T18:00:47.681412Z","shell.execute_reply":"2025-08-25T18:00:48.296645Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gradio/interface.py:414: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"🔗 ngrok public URL: NgrokTunnel: \"https://36940a0ba094.ngrok-free.app\" -> \"http://localhost:7860\"\n* Running on local URL:  http://0.0.0.0:7860\n* To create a public link, set `share=True` in `launch()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"\nTest edilecek görüntü: /tmp/gradio/fc8f49a596d176e3873c4e55c3de534b54894f142f431b8428d70500213f47fb/29503-TZi1PRD7j3MIPEqJ.jpg\nAltyazı üretiliyor...\nÜretilen Altyazı: bir banyoda tuvalet ve lavabo\n\nTest edilecek görüntü: /tmp/gradio/71c4edbdbba2b7dc080294d0778b308df6a6d8d5339bafaa2aad62b7dd504705/IMG_1240.JPG\nAltyazı üretiliyor...\nÜretilen Altyazı: bir adam bir masada oturuyor\n\nTest edilecek görüntü: /tmp/gradio/71c4edbdbba2b7dc080294d0778b308df6a6d8d5339bafaa2aad62b7dd504705/IMG_1240.JPG\nAltyazı üretiliyor...\nÜretilen Altyazı: Bir adam bir video oyunu oynuyor .\n\nTest edilecek görüntü: /tmp/gradio/71c4edbdbba2b7dc080294d0778b308df6a6d8d5339bafaa2aad62b7dd504705/IMG_1240.JPG\nAltyazı üretiliyor (Transformer ile)...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Üretilen Altyazı: bir adam bir makas tutuyor ve bir şeyler yiyor\n\nTest edilecek görüntü: /tmp/gradio/71c4edbdbba2b7dc080294d0778b308df6a6d8d5339bafaa2aad62b7dd504705/IMG_1240.JPG\nAltyazı üretiliyor (Transformer ile)...\nÜretilen Altyazı: bir adam bir makas tutuyor ve bir şeyler yiyor\n\nTest edilecek görüntü: /tmp/gradio/dd09b2ebcccdb6399a42b8f28daa1d8bf6d023fbe7b34e117e44682074181b04/IMG_3075.JPG\nAltyazı üretiliyor...\nÜretilen Altyazı: bir sahilde uçurtma uçuran bir kişi\n\nTest edilecek görüntü: /tmp/gradio/2866f401f3c2adde6b403a5a24ebcc002d0645c1000767c539f330260d4c05bb/IMG_9822.JPG\nAltyazı üretiliyor...\nÜretilen Altyazı: bir tekne suda bir tekne ve bir tekne\n\nTest edilecek görüntü: /tmp/gradio/2866f401f3c2adde6b403a5a24ebcc002d0645c1000767c539f330260d4c05bb/IMG_9822.JPG\nAltyazı üretiliyor...\nÜretilen Altyazı: bir su kütlesinde bir tekne\n","output_type":"stream"}],"execution_count":33}]}